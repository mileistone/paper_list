##### 2020
- [ECCV2020] [AssembleNet++: Assembling Modality Representations via Attention Connections](https://arxiv.org/abs/2008.08072)
- [CVPR2020] [Temporal Pyramid Network for Action Recognition](https://arxiv.org/abs/2004.03548)
  - TPN
  - training
    - data augmentation
      - temporal
        - random cropping
          - the input frames are sampled from a set of consecutive 64 frames at a specific interval
      - spatial
        - scale jittering
          - each frame is randomly cropped so that its short side ranges in [256, 320] pixels
        - horizontal random flip
    - regularization
      - dropout
        - 0.5
      - BN
        - enabled
  - testing
    - three-crop testing refers to three random crops of size 256×256 from the original frames, which are resized firstly to have 256 pixels in their shorter sides
    - ten-crop testing, which extracts 5 crops of size 224×224 and flips these crops
    - uniformly sample 10 clips of the whole video and average the softmax probabilities of all clips as the final
prediction
- [2012.06567] [A Comprehensive Study of Deep Video Action Recognition](https://arxiv.org/abs/2012.06567)
- [2004.04730] [X3D: Expanding Architectures for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)
  - X3D
  - similar to EfficientDet (compound scaling of input size, width, depth), MobileNet (block)
  - training
    - data augmentation
      - temporal
        - random cropping
          - we randomly sample a clip from the full-length video
      - spatial
        - scale jittering and random cropping
          - we randomly crop 224×224 pixels from a video, or its horizontal flip, with a shorter side randomly sampled in [256, 320] pixels
    - regularization
      - dropout
        - before final classifier
        - 0.5
      - BN
        - enabled
  - testing
    -  Temporally, uniformly samples K clips (e.g. K=10) from a video and spatially scales the shorter spatial side to specific pixels and takes a center crop
- [2002.07442] [V4D:4D Convolutional Neural Networks for Video-level Representation Learning](https://arxiv.org/abs/2002.07442)
  - Psudo 4D, bad paper
- [2001.06499] [Temporal Interlacing Network](https://arxiv.org/abs/2001.06499)
  - TIN
##### 2019
- [CVPR2019] [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)
  - SlowFast
  - training
    - data augmentation
      - temporal
        - random cropping
          - we randomly sample a clip from the full-length video
      - spatial
        - scale jittering and random cropping
          - we randomly crop 224×224 pixels from a video, or its horizontal flip, with a shorter side randomly sampled in [256, 320] pixels
      - regularization
        - dropout
          - before final classifier
          - 0.5
        - BN
          - enabled
  - testing
    - we uniformly sample 10 clips from a video along its temporal axis
    - for each clip, we scale the shorter spatial side to 256 pixels and take 3 crops of 256×256 to cover the spatial dimensions, as an approximation of fully-convolutional testing
    - average the softmax scores for prediction
  
- [ICCV2019] [Video Classification with Channel-Separated Convolutional Networks](https://arxiv.org/abs/1904.02811)
  - CSN
    - conv vs depthwise vs group conv vs space-time decomposition
  - training
    - data augmentation
      - temporal
        - random cropping
          - randomly select starting frame
      - spatial
        - scale jittering
        - random cropping

  - testing
    - center crops of 10 clips uniformly sampled from the video
    - average these 10 clip predictions to obtain the final video prediction
- [ICCV2019] [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/abs/1811.08383)
  - TSM
  - accurate and efficient
  - 2D-Conv equipped with TSM has the ability of temporal modeling
  - training
    - regularization
      - dropout
        - 0.5
      - BN
        - frozen
- [1905.13209] [AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures](https://arxiv.org/abs/1905.13209)
##### 2018
- [ECCV2018] [Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification](https://arxiv.org/abs/1712.04851)
  - S3D
    - mix 2D-Conv with 3D-Conv
    - feature gating (SENet)
  - training
    - data augmentation
      - temporal
        - random cropping
          - densely sample 64 frames from a video
      - spatial
        - random cropping
          - resize input frames to 256×256, randomly crop a 224×224 patch
  - testing
    - use all frames and take 224×224 center crops from the resized frames
- [ECCV2018] [Temporal Relational Reasoning in Videos](https://arxiv.org/abs/1711.08496)
  - TRN
- [CVPR2018] [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248)
  - R(2+1)D
  - training
    - data augmentation
      - temporal
        - random cropping
          - randomly sample L consecutive frames from the video with temporal jittering
      - spatial
        - random cropping
          - frames are scaled to the size of 128×171 and then each clip is generated by randomly cropping windows of size 112×112
    - use the first 10 epochs for warmup
  - testing
    - use center crops of 10 clips uniformly sampled from the video and average these 10 clip predictions to obtain the video prediction
- [CVPR2018] [What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets](http://ai.stanford.edu/~dahuang/papers/cvpr18-fb.pdf)
  - How much motion matters
- [CVPR2018] [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)
  - Non-local
  - training
    - data augmentation
      - temporal
        - random cropping
          - using 32-frame input clips which are formed by randomly cropping out 64 consecutive frames from the original full-length video and then dropping every other frame
      - spatial
        - scale jittering and random cropping
          - the spatial size is 224×224 pixels, randomly cropped from a scaled video whose shorter side is randomly sampled in [256, 320] pixels
    - regularization
      - dropout
        - after the global pooling layer
        - 0.5
      - BN
        - enabled
  - testing
    - perform spatially fully convolutional inference on videos whose shorter side is rescaled to 256
    - for the temporal domain, in our practice we sample 10 clips evenly from a full-length video and compute the softmax scores on them individually
    - the final prediction is the averaged softmax scores of all clips.
##### 2017
- [ICCV2017] [Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks](https://openaccess.thecvf.com/content_ICCV_2017/papers/Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper.pdf)
  - P3D 
    - comparison with 2D ResNet
  - training
    - data augmentation
      - temporal 
        - random cropping
          - 16-frame snippets
      - sptial
        - random cropping
          - resize the smaller video side to 256 pixels, then randomly crop a 224×224 patch
        - horizontal random flip
    - regularization
      - dropout
        - 0.9 for UCF101
        - 0.1 for Sports-1M
      - BN
        - frozen
  - testing
    - the models are applied convolutionally over the whole video taking 224×224 center crops, and the predictions are averaged
      - we briefly tried spatially-convolutional testing on the 256×256 videos, but did not observe improvement
- [CVPR2017] [Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750)
  - Kinetics
  - I3D
    - LSTM vs Two-Stream vs 3D-ConvNet vs 3D-Fused vs Two-Stream I3D
  - training
    - data augmentation
      - temporal
        - random cropping
          - 64-frame snippets
      - spatial
        - random cropping
          - resize the smaller video side to 256 pixels, then randomly crop a 224×224 patch
        - horizontal random flip
  - testing
    - the models are applied convolutionally over the whole video taking 224×224 center crops, and the predictions are averaged
      - We briefly tried spatially-convolutional testing on the 256×256 videos, but did not observe improvement
      
##### 2016
- [ECCV2016] [Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/abs/1608.00859)
  - TSN
    - Two-Stream
  - training
    - great modeling capacity
    - data augmentation
      - spatial
        - corner cropping
        - scale jittering
        - horizontal random flip
    - regularization
      - partial BN with dropout
      - pre-training
      - cross modality pre-training
    - multi-modality
      - RGB difference
      - optical flow
      - warped optical flow
  - testing
    - given a video, we sample a fixed number of frames (25 in our experiments) with equal temporal spacing between them
    - cropping and flipping four corners and the center of the frame
    
##### 2015
- [ICCV2015] [Learning Spatiotemporal Features with 3D Convolutional Networks](https://arxiv.org/abs/1412.0767)
  - C3D
  - training
    - data augmentation
      - temporal
        - random cropping
          - 16-frame snippets
          - randomly extract five 2-second long clips from every training video
      - spatial
        - random cropping
          - resize to have a frame size of 128×171, then randomly crop a 112×112 patch
        - horizontal random flip
- [1507.02159] [Towards Good Practices for Very Deep Two-Stream ConvNet](https://arxiv.org/abs/1507.02159)
  - Two-Stream
  - training
    - data augmentation
      - spatial
        - corner cropping
        - scale jittering
        - random horizontal flip
    - regularization
      - pre-training
      - dropout
        - 0.9 or o.8
    - small learning rate
  - testing
    - given a video, we sample a fixed number of frames (25 in our experiments) with equal temporal spacing between them
    - cropping and flipping four corners and the center of the frame
##### 2014
- [NIPS2014] [Two-Stream Convolutional Networks for Action Recognition in Videos](https://papers.nips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf)
  - Two-Stream
    - RGB
    - optical flow
  - training
    - data augmentation
      - spatial
        - random cropping
        - random horizontal flip
      - pixel
        - color jittering
    - regularization
      - pre-training
      - high dropout ratio
  - testing
    - given a video, we sample a fixed number of frames (25 in our experiments) with equal temporal spacing between them
    - crop and flip four corners and the center of the frame
- [CVPR2014] [Large-scale Video Classification with Convolutional Neural Networks](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/42455.pdf)
  - Sports-1M
